{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"./mapalign\")\n",
    "from mapalign import embed\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pli\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"/n02dat01/users/ypwang/Gradient/GeneticGradient/Data/FunctionalGradients/\") \n",
    "os.chdir(\"/n02dat01/users/ypwang/Gradient/GeneticGradient/Data/FG_noBrainSmash/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cupy as cp\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "start = time.time()\n",
    "mempool = cp.get_default_memory_pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Self corrected function from mapalign because version conflict I think\n",
    "def compute_diffusion_map_yp(L, alpha=0.5, n_components=None, diffusion_time=0,\n",
    "                          skip_checks=False, overwrite=False):\n",
    "    \"\"\"Compute the diffusion maps of a symmetric similarity matrix\n",
    "\n",
    "        L : matrix N x N\n",
    "           L is symmetric and L(x, y) >= 0\n",
    "\n",
    "        alpha: float [0, 1]\n",
    "            Setting alpha=1 and the diffusion operator approximates the\n",
    "            Laplace-Beltrami operator. We then recover the Riemannian geometry\n",
    "            of the data set regardless of the distribution of the points. To\n",
    "            describe the long-term behavior of the point distribution of a\n",
    "            system of stochastic differential equations, we can use alpha=0.5\n",
    "            and the resulting Markov chain approximates the Fokker-Planck\n",
    "            diffusion. With alpha=0, it reduces to the classical graph Laplacian\n",
    "            normalization.\n",
    "\n",
    "        n_components: int\n",
    "            The number of diffusion map components to return. Due to the\n",
    "            spectrum decay of the eigenvalues, only a few terms are necessary to\n",
    "            achieve a given relative accuracy in the sum M^t.\n",
    "\n",
    "        diffusion_time: float >= 0\n",
    "            use the diffusion_time (t) step transition matrix M^t\n",
    "\n",
    "            t not only serves as a time parameter, but also has the dual role of\n",
    "            scale parameter. One of the main ideas of diffusion framework is\n",
    "            that running the chain forward in time (taking larger and larger\n",
    "            powers of M) reveals the geometric structure of X at larger and\n",
    "            larger scales (the diffusion process).\n",
    "\n",
    "            t = 0 empirically provides a reasonable balance from a clustering\n",
    "            perspective. Specifically, the notion of a cluster in the data set\n",
    "            is quantified as a region in which the probability of escaping this\n",
    "            region is low (within a certain time t).\n",
    "\n",
    "        skip_checks: bool\n",
    "            Avoid expensive pre-checks on input data. The caller has to make\n",
    "            sure that input data is valid or results will be undefined.\n",
    "\n",
    "        overwrite: bool\n",
    "            Optimize memory usage by re-using input matrix L as scratch space.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "\n",
    "        [1] https://en.wikipedia.org/wiki/Diffusion_map\n",
    "        [2] Coifman, R.R.; S. Lafon. (2006). \"Diffusion maps\". Applied and\n",
    "        Computational Harmonic Analysis 21: 5-30. doi:10.1016/j.acha.2006.04.006\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import scipy.sparse as sps\n",
    "\n",
    "    use_sparse = False\n",
    "    if sps.issparse(L):\n",
    "        use_sparse = True\n",
    "\n",
    "    if not skip_checks:\n",
    "        # Original: from sklearn.manifold.spectral_embedding_ import _graph_is_connected\n",
    "        from sklearn.manifold._spectral_embedding import _graph_is_connected\n",
    "        # from sklearn.manifold.spectral_embedding import _graph_is_connected\n",
    "        if not _graph_is_connected(L):\n",
    "            raise ValueError('Graph is disconnected')\n",
    "\n",
    "    ndim = L.shape[0]\n",
    "    if overwrite:\n",
    "        L_alpha = L\n",
    "    else:\n",
    "        L_alpha = L.copy()\n",
    "\n",
    "    if alpha > 0:\n",
    "        # Step 2\n",
    "        d = np.array(L_alpha.sum(axis=1)).flatten()       # 行相加求和：axis=0, 表示列。axis=1, 表示行. flatten: 返回一个一维数组。\n",
    "        d_alpha = np.power(d, -alpha)                     # 0.5次方：np.power 求n次方\n",
    "        if use_sparse:\n",
    "            L_alpha.data *= d_alpha[L_alpha.indices]\n",
    "            L_alpha = sps.csr_matrix(L_alpha.transpose().toarray())\n",
    "            L_alpha.data *= d_alpha[L_alpha.indices]\n",
    "            L_alpha = sps.csr_matrix(L_alpha.transpose().toarray())\n",
    "        else:\n",
    "            L_alpha = d_alpha[:, np.newaxis] * L_alpha    # 插入新维度\n",
    "            L_alpha = L_alpha * d_alpha[np.newaxis, :]\n",
    "\n",
    "    # Step 3\n",
    "    d_alpha = np.power(np.array(L_alpha.sum(axis=1)).flatten(), -1)\n",
    "    if use_sparse:\n",
    "        L_alpha.data *= d_alpha[L_alpha.indices]\n",
    "    else:\n",
    "        L_alpha = d_alpha[:, np.newaxis] * L_alpha\n",
    "\n",
    "    M = L_alpha\n",
    "\n",
    "    from scipy.sparse.linalg import eigsh, eigs\n",
    "\n",
    "    # Step 4\n",
    "    func = eigs\n",
    "    if n_components is not None:\n",
    "        lambdas, vectors = func(M, k=n_components + 1)\n",
    "    else:\n",
    "        # 求矩阵M的k个特征值和特征向量, \n",
    "        # lamadas：ndarray k个特征值的数组. \n",
    "        # vector：ndarray k个特征向量的数组. v[:, i]是对应于特征值w [i]的特征向量。\n",
    "        lambdas, vectors = func(M, k=max(2, int(np.sqrt(ndim))))        \n",
    "    del M\n",
    "\n",
    "    if func == eigsh:\n",
    "        lambdas = lambdas[::-1]\n",
    "        vectors = vectors[:, ::-1]\n",
    "    else:\n",
    "        lambdas = np.real(lambdas)\n",
    "        vectors = np.real(vectors)\n",
    "        lambda_idx = np.argsort(lambdas)[::-1]\n",
    "        lambdas = lambdas[lambda_idx]\n",
    "        vectors = vectors[:, lambda_idx]\n",
    "\n",
    "    return _step_5(lambdas, vectors, ndim, n_components, diffusion_time)\n",
    "\n",
    "def _step_5(lambdas, vectors, ndim, n_components, diffusion_time):\n",
    "    \"\"\"\n",
    "    This is a helper function for diffusion map computation.\n",
    "\n",
    "    The lambdas have been sorted in decreasing order.\n",
    "    The vectors are ordered according to lambdas.\n",
    "\n",
    "    \"\"\"\n",
    "    psi = vectors/vectors[:, [0]]\n",
    "    diffusion_times = diffusion_time\n",
    "    if diffusion_time == 0:\n",
    "        diffusion_times = np.exp(1. -  np.log(1 - lambdas[1:])/np.log(lambdas[1:]))\n",
    "        lambdas = lambdas[1:] / (1 - lambdas[1:])\n",
    "    else:\n",
    "        lambdas = lambdas[1:] ** float(diffusion_time)\n",
    "    lambda_ratio = lambdas/lambdas[0]\n",
    "    threshold = max(0.05, lambda_ratio[-1])\n",
    "\n",
    "    n_components_auto = np.amax(np.nonzero(lambda_ratio > threshold)[0])\n",
    "    n_components_auto = min(n_components_auto, ndim)\n",
    "    if n_components is None:\n",
    "        n_components = n_components_auto\n",
    "    embedding = psi[:, 1:(n_components + 1)] * lambdas[:n_components][None, :]\n",
    "\n",
    "    result = dict(lambdas=lambdas, vectors=vectors,\n",
    "                  n_components=n_components, diffusion_time=diffusion_times,\n",
    "                  n_components_auto=n_components_auto)\n",
    "    return embedding, result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Calculate the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Load FC matrix\n",
    "Grey_file = nib.load(\"HCP_S1200_1003_rfMRI_MSMAll_groupPCA_d4500ROW_zcorr.dconn.nii\")\n",
    "axis2 = Grey_file.header.get_axis(1)\n",
    "cerebellum_mask_ind = np.append(np.where(axis2.__dict__['_name'] == 'CIFTI_STRUCTURE_CEREBELLUM_LEFT'), \n",
    "                                np.where(axis2.__dict__['_name'] == 'CIFTI_STRUCTURE_CEREBELLUM_RIGHT'))\n",
    "# FC_all = nib.load('HCP_S1200_1003_rfMRI_MSMAll_groupPCA_d4500ROW_zcorr.dconn.nii').get_fdata() # 91282*91282\n",
    "# FC_cere2extra_1 = FC_all[cerebellum_mask_ind, :]\n",
    "# FC_cere2extra = np.delete(FC_cere2extra_1,cerebellum_mask_ind, axis=1)\n",
    "FC_filename = 'HCP_S1200_1003_rfMRI_MSMAll_groupPCA_d4500ROW_zcorr_cere2extra_17853'\n",
    "# np.save(FC_filename, FC_cere2extra) #17853*73429, \n",
    "\n",
    "## Calculate the gradient\n",
    "# dcon = np.tanh(np.load(str(FC_filename) + '.npy'))\n",
    "# perc = np.array([np.percentile(x, 90) for x in dcon])                     \n",
    "# for j in range(dcon.shape[0]):                                            \n",
    "#     print (\"Row %d\" % j)\n",
    "#     dcon[j, dcon[j,:] < perc[j]] = 0\n",
    "# dcon[dcon < 0] = 0\n",
    "# aff = 1 - pairwise_distances(dcon, metric = 'cosine')     \n",
    "# aff_filename = 'cosine_affinity_cere2extra_17853.npy'\n",
    "# np.save(aff_filename, aff)\n",
    "# aff = np.load(aff_filename)\n",
    "# emb, res = compute_diffusion_map_yp(aff, alpha = 0.5)      \n",
    "# emb_filename = 'embedding_dense_cere2extra_emb_17853.npy'     \n",
    "# res_filename = 'embedding_dense_cere2extra_res_17853.npy'           \n",
    "# np.save(emb_filename, emb)                   \n",
    "# np.save(res_filename, res)\n",
    "# # This is for Normalizetion\n",
    "# emb_i = -1 * emb\n",
    "# emb_i_filename = 'embedding_dense_cere2extra_emb_i_17853.npy'   \n",
    "# np.save(emb_i_filename, emb_i) \n",
    "\n",
    "# emb_z = pd.DataFrame()\n",
    "# emb_s = pd.DataFrame()\n",
    "# for j in range(len(emb.T)):\n",
    "#     print(j)\n",
    "#     _range = emb_i.T[j].max() - emb_i.T[j].min()\n",
    "#     emb_z[j] = (emb_i.T[j] -  emb_i.T[j].min()) / _range\n",
    "#     emb_s[j] = ((emb_i.T[j] -  emb_i.T[j].min()) / _range)+1\n",
    "# emb_z_filename = 'embedding_dense_cere2extra_emb_z_17853.npy'   \n",
    "# np.save(emb_z_filename, emb_z.values) \n",
    "# emb_s_filename = 'embedding_dense_cere2extra_emb_s_17853.npy'   \n",
    "# np.save(emb_s_filename, emb_s.values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Show the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using Suitpy\n",
    "import SUITPy as suit\n",
    "plot_data = suit.flatmap.vol_to_surf('result_cere2extra_gradient1__z_17853_nifti.nii',space='SUIT', stats='nanmean')\n",
    "suit.flatmap.plot(data=plot_data, new_figure=True, colorbar=True, cmap='jet') # , cscale=[np.min(plot_data), np.max(plot_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Save out and show\n",
    "from tqdm import tqdm\n",
    "for emb_idx in ['', '_i', '_z', '_s']:\n",
    "    print(emb_idx)\n",
    "    emb_name = 'embedding_dense_cere2extra_emb' + str(emb_idx) + '_17853.npy'\n",
    "    emb_cur = np.load(emb_name)\n",
    "    cope1_file = nib.load(\"cope1.dtseries.nii\")\n",
    "    output = np.zeros([91282])\n",
    "    for j in tqdm(range(len(emb_cur.T))):\n",
    "        dscalar_name = 'result_cere2extra_gradient' + str(j+1) + '_' + str(emb_idx) + '_17853.dscalar.nii'\n",
    "        nifti_name = 'result_cere2extra_gradient' + str(j+1) + '_' + str(emb_idx) + '_17853_nifti.nii'\n",
    "        image_name = 'result_cere2extra_gradient' + str(j+1) + '_' + str(emb_idx) + '_17853.png'\n",
    "        pdf_name = 'result_cere2extra_gradient' + str(j+1) + '_' + str(emb_idx) + '_17853.pdf'\n",
    "        \n",
    "        # dscalar file\n",
    "        output[cerebellum_mask_ind] = emb_cur [:,j]\n",
    "        img_test = nib.cifti2.Cifti2Image(output.reshape([1,-1]),cope1_file.header)\n",
    "        img_test.to_filename(dscalar_name)\n",
    "        \n",
    "        # nifti file\n",
    "        cmd_5 = \"wb_command -cifti-separate \" + str(dscalar_name) + \" COLUMN -volume-all \" + str(nifti_name)\n",
    "        print(cmd_5)\n",
    "        subprocess.check_output(cmd_5, shell=True);\n",
    "        \n",
    "        # show in png\n",
    "        cmd_6 = \"scp \" + str(nifti_name) +' image_cerebellumonly_nifti.nii'\n",
    "        subprocess.check_output(cmd_6, shell=True);\n",
    "        subprocess.check_output('bash call_matlab_cerebellumonly.sh', shell=True);\n",
    "        image = plt.imread('image_cerebellumonly.png')  \n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        \n",
    "        # rename and svae in pdf\n",
    "        cmd_7 = \"scp image_cerebellumonly.png \" + str(image_name)\n",
    "        subprocess.check_output(cmd_7, shell=True);\n",
    "        cmd_8 = \"convert image_cerebellumonly.png \" + str(pdf_name)\n",
    "        subprocess.check_output(cmd_8, shell=True);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Save out and show\n",
    "# for emb_idx in ['', '_i', '_z', '_s']:\n",
    "emb_idx = '_z'\n",
    "print(emb_idx)\n",
    "emb_name = 'embedding_dense_cere2extra_emb' + str(emb_idx) + '_17853.npy'\n",
    "emb_cur = np.load(emb_name)\n",
    "cope1_file = nib.load(\"cope1.dtseries.nii\")\n",
    "output = np.zeros([91282])\n",
    "# for j in range(len(emb_cur.T)):\n",
    "j = 0\n",
    "dscalar_name = 'result_cere2extra_gradient' + str(j+1) + '_' + str(emb_idx) + '_17853.dscalar.nii'\n",
    "nifti_name = 'result_cere2extra_gradient' + str(j+1) + '_' + str(emb_idx) + '_17853_nifti.nii'\n",
    "image_name = 'result_cere2extra_gradient' + str(j+1) + '_' + str(emb_idx) + '_17853.png'\n",
    "pdf_name = 'result_cere2extra_gradient' + str(j+1) + '_' + str(emb_idx) + '_17853.pdf'\n",
    "\n",
    "# dscalar file\n",
    "output[cerebellum_mask_ind] = emb_cur [:,j]\n",
    "img_test = nib.cifti2.Cifti2Image(output.reshape([1,-1]),cope1_file.header)\n",
    "img_test.to_filename(dscalar_name)\n",
    "\n",
    "# nifti file\n",
    "cmd_5 = \"wb_command -cifti-separate \" + str(dscalar_name) + \" COLUMN -volume-all \" + str(nifti_name)\n",
    "print(cmd_5)\n",
    "subprocess.check_output(cmd_5, shell=True);\n",
    "\n",
    "# show in png\n",
    "cmd_6 = \"scp \" + str(nifti_name) +' image_cerebellumonly_nifti.nii'\n",
    "subprocess.check_output(cmd_6, shell=True);\n",
    "subprocess.check_output('bash call_matlab_cerebellumonly.sh', shell=True);\n",
    "image = plt.imread('image_cerebellumonly.png')  \n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# rename and svae in pdf\n",
    "cmd_7 = \"scp image_cerebellumonly.png \" + str(image_name)\n",
    "subprocess.check_output(cmd_7, shell=True);\n",
    "cmd_8 = \"convert image_cerebellumonly.png \" + str(pdf_name)\n",
    "subprocess.check_output(cmd_8, shell=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Confirm SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantiative demonstation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from scipy import stats\n",
    "from nilearn import datasets, image, input_data, plotting\n",
    "import scipy\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "git_dir = '/n02dat01/users/ypwang/Gradient/GeneticGradient/Scripts/'\n",
    "import sys\n",
    "sys.path.insert(0,git_dir)\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport FunctionLibrary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB_to_Hex(rgb):\n",
    "    RGB = rgb.split(',')\n",
    "    color = '#'\n",
    "    for i in RGB:\n",
    "        num = int(i)\n",
    "        color += str(hex(num))[-2:].replace('x', '0').upper()\n",
    "    return color\n",
    "yeo_dir = '/n02dat01/users/ypwang/Gradient/Tool/Atlas/Yeo2011_fcMRI_clustering/1000subjects_reference/'  \n",
    "\n",
    "# color_7 = pd.read_csv(\"/n01dat01/ypwang/AHBA/CerebellarGeneFCCorrelation/Reference_files/7NetworksColors.csv\")\n",
    "mat_struct = scipy.io.loadmat(f\"{yeo_dir}/7NetworksColors.mat\")\n",
    "color_7 = pd.DataFrame(mat_struct['colors']).drop(0)\n",
    "color_7.columns = ['R', 'G', 'B']\n",
    "index_7 = pd.read_csv(\"/n01dat01/ypwang/AHBA/CerebellarGeneFCCorrelation/Reference_files/7network_names.csv\")\n",
    "# index_7 = pd.read_csv(f\"{yeo_dir}/7NetworksOrderedNames.csv\", index_col=0)\n",
    "color_7_hex = [RGB_to_Hex(str(tuple(color_7.iloc[i,:])).replace(\"(\",\"\").replace(\")\",\"\")) for i in range(len(color_7))]\n",
    "color_7_hex = pd.DataFrame(color_7_hex)\n",
    "color_7.index = color_7_hex.index \n",
    "color_7 = pd.merge(color_7_hex,color_7,left_index=True,right_index=True)\n",
    "color_7 = pd.merge(index_7,color_7,left_index=True,right_index=True)\n",
    "color_7.columns = ['Name', 'Hex', 'R', 'G', 'B']\n",
    "\n",
    "order = ['SomMot', 'Vis', 'DorsAttn', 'VentAttn', 'Cont', 'Limbic', 'Default']\n",
    "    \n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "Net_7 = nib.load(\"/n02dat01/users/ypwang/SCZ/SCZ_202310/Data/Info/Atlas/yeo_atlas/cere_2mm_17.nii.gz\")\n",
    "Net_7 = NiftiLabelsMasker(labels_img=Net_7, standardize=False, strategy=\"mean\")\n",
    "Net_7.fit()\n",
    "Net_7_data = []\n",
    "\n",
    "mat_struct = scipy.io.loadmat(f\"{yeo_dir}/7NetworksColors.mat\")\n",
    "color_7 = pd.DataFrame(mat_struct['colors']).drop(0)\n",
    "color_7.columns = ['R', 'G', 'B']\n",
    "index_7 = pd.read_csv(\"/n01dat01/ypwang/AHBA/CerebellarGeneFCCorrelation/Reference_files/7network_names.csv\")\n",
    "# index_7 = pd.read_csv(f\"{yeo_dir}/7NetworksOrderedNames.csv\", index_col=0)\n",
    "color_7_hex = [RGB_to_Hex(str(tuple(color_7.iloc[i,:])).replace(\"(\",\"\").replace(\")\",\"\")) for i in range(len(color_7))]\n",
    "color_7_hex = pd.DataFrame(color_7_hex)\n",
    "color_7.index = color_7_hex.index \n",
    "color_7 = pd.merge(color_7_hex,color_7,left_index=True,right_index=True)\n",
    "color_7 = pd.merge(index_7,color_7,left_index=True,right_index=True)\n",
    "color_7.columns = ['Name', 'Hex', 'R', 'G', 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "net = 7\n",
    "atlas_file = nib.load(f'/n14dat01/lchai/HCP_MIN/cere_2mm_{net}.nii.gz')\n",
    "label_values = np.unique(atlas_file.get_fdata())\n",
    "nii_file = nib.load('result_cerebellumonly_gradient1__17853_nifti.nii')\n",
    "FG_net7 = nii_file.get_fdata()\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for x in label_values:\n",
    "    mask = (atlas_file.get_fdata() == x)\n",
    "    voxel_data = FG_net7[mask]\n",
    "    label_data = np.repeat(x, np.sum(mask))\n",
    "    FG_cur = pd.DataFrame({'Voxel Data': voxel_data, 'Label': label_data})\n",
    "    \n",
    "    # Concatenate the current DataFrame with the result DataFrame\n",
    "    result_df = pd.concat([result_df, FG_cur])\n",
    "\n",
    "label_name_mapping = {\n",
    "    1: 'Vis',\n",
    "    2: 'SomMot',\n",
    "    3: 'DorsAttn',\n",
    "    4: 'VentAttn',\n",
    "    5: 'Limbic',\n",
    "    6: 'Cont',\n",
    "    7: 'Default'\n",
    "}\n",
    "\n",
    "# Add a new column 'label_name' based on the mapping\n",
    "result_df['label_name'] = result_df['Label'].map(label_name_mapping)\n",
    "# Boxplot code\n",
    "order_cur = ['SomMot', 'Vis', 'DorsAttn', 'VentAttn', 'Cont', 'Limbic', 'Default']\n",
    "# color_7 = [color_7.loc[color_7.loc[:,'Name']==i,'Hex'].squeeze() for i in order_cur]\n",
    "color_reorder_cur = [color_7.loc[color_7.loc[:, 'Name'] == i, 'Hex'].squeeze() for i in order_cur]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "bx = ax.boxplot([result_df[result_df.loc[:, 'label_name'] == i].loc[:, 'Voxel Data'].values for i in order_cur],\n",
    "                labels=order_cur, notch=0, vert=True, patch_artist=True, showfliers=False, showmeans=True,\n",
    "                meanprops={\"marker\": \"D\", \"markerfacecolor\": \"white\", \"linewidth\": 2, \"markeredgecolor\": \"grey\"},\n",
    "                medianprops={\"color\": \"white\", \"linewidth\": 1},\n",
    "                whiskerprops={\"linewidth\": 2.5},\n",
    "                capprops={\"linewidth\": 3})\n",
    "\n",
    "for index, bp in enumerate(bx[\"boxes\"]):\n",
    "    bp.set_facecolor(color_reorder_cur[index])\n",
    "    bp.set_edgecolor(color_reorder_cur[index])\n",
    "    bp.set_alpha(0.8)\n",
    "\n",
    "for index, bp in enumerate(bx[\"whiskers\"]):\n",
    "    bp.set_color(color_reorder_cur[int(index / 2)])\n",
    "    bp.set_alpha(0.8)\n",
    "\n",
    "for index, bp in enumerate(bx[\"caps\"]):\n",
    "    bp.set_color(color_reorder_cur[int(index / 2)])\n",
    "    bp.set_alpha(0.8)\n",
    "\n",
    "ax.axhline(0, linestyle='--', color='grey')  # , color='k'\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.set_ylabel(\"FG\", fontsize=20)\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "# ax.set_xlabel('Networks', fontsize=16)\n",
    "ax.spines['bottom'].set_linewidth(1.6)\n",
    "ax.spines['left'].set_linewidth(1.6)\n",
    "\n",
    "plt.savefig('./R2.2/FG_Network_7.png', dpi=700, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB_to_Hex(rgb):\n",
    "    RGB = rgb.replace(\"'\", '').split(',')\n",
    "    color = '#'\n",
    "    for i in RGB:\n",
    "        num = int(i)\n",
    "        color += str(hex(num))[-2:].replace('x', '0').upper()\n",
    "    return color\n",
    "\n",
    "data = []\n",
    "with open(\"/n02dat01/users/ypwang/AHBA/Github_20220301/Reference_files/MDTB_10Regions_Color.txt\", 'r', encoding='utf8') as f:\n",
    "    for i in f:\n",
    "        data.append([j for j in i.split()])\n",
    "\n",
    "color_10 = pd.DataFrame(data).iloc[:, 1:]\n",
    "color_10.columns = ['R', 'G', 'B']\n",
    "index_10 = pd.read_csv(\"/n02dat01/users/ypwang/AHBA/Github_20220301/Reference_files/MDTB_10network_names.CSV\")\n",
    "color_10_hex = [RGB_to_Hex(','.join(color_10.iloc[i, :])) for i in range(len(color_10))]\n",
    "color_10_hex = pd.DataFrame(color_10_hex)\n",
    "color_10.index = color_10_hex.index\n",
    "color_10 = pd.merge(color_10_hex, color_10, left_index=True, right_index=True)\n",
    "color_10 = pd.merge(index_10, color_10, left_index=True, right_index=True)\n",
    "color_10.columns = ['Name', 'Hex', 'R', 'G', 'B']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "atlas_file = nib.load(\"/n02dat01/users/ypwang/SCZ/SCZ_202310/Data/Info/Atlas/MDTB_atlas/MDTB_10Regions_MNI_2MM.nii.gz\")\n",
    "label_values = np.unique(atlas_file.get_fdata())\n",
    "nii_file = nib.load('result_cerebellumonly_gradient1__17853_nifti.nii')\n",
    "FG_net10 = nii_file.get_fdata()\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for x in label_values:\n",
    "    mask = (atlas_file.get_fdata() == x)\n",
    "    voxel_data = FG_net10[mask]\n",
    "    label_data = np.repeat(x, np.sum(mask))\n",
    "    FG_cur = pd.DataFrame({'VoxelData': voxel_data, 'Label': label_data})\n",
    "    \n",
    "    # Concatenate the current DataFrame with the result DataFrame\n",
    "    result_df = pd.concat([result_df, FG_cur])\n",
    "\n",
    "label_name_mapping = {\n",
    "    1: 'Left_Hand_Press',\n",
    "    2: 'Right_Hand_Press',\n",
    "    3: 'Saccades',\n",
    "    4: 'Action_observation',\n",
    "    5: 'Divided_Attention_Mental',\n",
    "    6: 'Divided_Attention_Verbal',\n",
    "    7: 'Narrative',\n",
    "    8: 'Word_Comprehension',  # 请根据实际情况修改\n",
    "    9: 'Verbal_Fluency',\n",
    "    10: 'Autobiographical_Recall'\n",
    "}\n",
    "\n",
    "# Add a new column 'label_name' based on the mapping\n",
    "result_df['label_name'] = result_df['Label'].map(label_name_mapping)\n",
    "result_df['label_name'] = result_df['label_name'].astype(str)\n",
    "order_df = pd.DataFrame(np.zeros([len(np.unique(result_df.loc[:, 'label_name'])), len(result_df.T)]))\n",
    "order_df.index = np.unique(result_df.label_name)\n",
    "order_df['median'] = [np.median(result_df.loc[result_df.loc[:,'label_name'] == x,'VoxelData']) for x in np.unique(result_df.label_name)]\n",
    "# print(order_df.sort_values(by='mean', ascending=False))\n",
    "order_cur = order_df.sort_values(by='median', ascending=True).index.values\n",
    "order_cur = order_cur[order_cur != 'nan']\n",
    "color_reorder_cur = [color_10.loc[color_10.loc[:,'Name']==i,'Hex'].squeeze() for i in order_cur]\n",
    "# Boxplot code\n",
    "# order_cur = ['SomMot', 'Vis', 'DorsAttn', 'VentAttn', 'Cont', 'Limbic', 'Default']\n",
    "# color_7 = [color_7.loc[color_7.loc[:,'Name']==i,'Hex'].squeeze() for i in order_cur]\n",
    "# color_10 = [color_7.loc[color_7.loc[:, 'Name'] == i, 'Hex'].squeeze() for i in order_cur]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "bx = ax.boxplot([result_df[result_df.loc[:, 'label_name'] == i].loc[:, 'VoxelData'].values for i in order_cur],\n",
    "                labels=[label.replace('_', '\\n') for label in order_cur], notch=0, vert=True, patch_artist=True, showfliers=False, showmeans=True,\n",
    "                meanprops={\"marker\": \"D\", \"markerfacecolor\": \"white\", \"linewidth\": 2, \"markeredgecolor\": \"grey\"},\n",
    "                medianprops={\"color\": \"white\", \"linewidth\": 1},\n",
    "                whiskerprops={\"linewidth\": 2.5},\n",
    "                capprops={\"linewidth\": 3})\n",
    "\n",
    "for index, bp in enumerate(bx[\"boxes\"]):\n",
    "    bp.set_facecolor(color_reorder_cur[index])\n",
    "    bp.set_edgecolor(color_reorder_cur[index])\n",
    "    bp.set_alpha(0.8)\n",
    "\n",
    "for index, bp in enumerate(bx[\"whiskers\"]):\n",
    "    bp.set_color(color_reorder_cur[int(index / 2)])\n",
    "    bp.set_alpha(0.8)\n",
    "\n",
    "for index, bp in enumerate(bx[\"caps\"]):\n",
    "    bp.set_color(color_reorder_cur[int(index / 2)])\n",
    "    bp.set_alpha(0.8)\n",
    "\n",
    "ax.axhline(0, linestyle='--', color='grey')  # , color='k'\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.set_ylabel(\"FG\", fontsize=20)\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "# ax.set_xlabel('Networks', fontsize=16)\n",
    "ax.spines['bottom'].set_linewidth(1.6)\n",
    "ax.spines['left'].set_linewidth(1.6)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='center', va='top', multialignment='right')\n",
    "plt.savefig('./R2.2/FG_Network_10.png', dpi=700, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 cerebellar FG sorted ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disbtw(surf, aa):\n",
    "    surf= np.array(surf)\n",
    "    aa = np.expand_dims(aa,axis=0)\n",
    "    dis = np.sqrt(((surf-aa)**2).sum(axis=1))\n",
    "    return dis\n",
    "\n",
    "def Grid2world(GridList, affine):\n",
    "    \"\"\"\n",
    "    trans the grid coords into the corresponding MNI coords in MNI space\n",
    "    GridList shape: N*3, affine shape 4*4\n",
    "    \"\"\"\n",
    "    GridList = np.array(GridList)\n",
    "    length = len(GridList)\n",
    "    affine = np.array(affine)\n",
    "    Grid_coord = np.concatenate([GridList, np.ones([length, 1])], axis=1)\n",
    "    MNI_coord = Grid_coord.dot(affine.T)      \n",
    "    MNI_coord = MNI_coord.round(2)\n",
    "    return MNI_coord[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "# Load the gradient file\n",
    "gradient_file = nib.load('result_cerebellumonly_gradient1__17853_nifti.nii')\n",
    "gradient = gradient_file.get_data()\n",
    "affine = gradient_file.affine\n",
    "\n",
    "# Find non-zero values and their indices in the gradient\n",
    "grid_list = np.nonzero(gradient)\n",
    "grad_values = gradient[grid_list]\n",
    "\n",
    "# Split gradient values into 10 bins\n",
    "bins = np.linspace(np.min(grad_values), np.max(grad_values), 100+1)\n",
    "digitized = np.digitize(grad_values, bins)\n",
    "\n",
    "# Create 10 ROIs based on the bins\n",
    "\n",
    "for i in range(1, 100+1):\n",
    "    # Get indices for the current bin\n",
    "    bin_indices = grid_list[0][digitized == i], grid_list[1][digitized == i], grid_list[2][digitized == i]\n",
    "\n",
    "    # Create an empty array for the ROI\n",
    "    roi = np.zeros_like(gradient)\n",
    "\n",
    "    # Set the values in the ROI array to a specific range (e.g., 1 to 10)\n",
    "    roi[bin_indices] = i\n",
    "    # Fill the ROI array with values from the current bin\n",
    "    # roi[bin_indices] = gradient[bin_indices]\n",
    "\n",
    "    # Create a new NIfTI file for the ROI\n",
    "    roi_nifti = nib.Nifti1Image(roi, affine)\n",
    "    nib.save(roi_nifti, f'./R2.2/roi_100_{i}.nii')\n",
    "\n",
    "# load nii to get coords\n",
    "\n",
    "for i in range(1, 100+1):\n",
    "    data = nib.load(f'./R2.2/roi_100_{i}.nii').get_fdata()\n",
    "    coord = np.array(np.where(data==i)).T\n",
    "    pd.DataFrame(coord).to_csv(f'./R2.2/roi_100_{i}_res2mm_grid.txt', sep='\\t', index=False, header=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the list for matlab type input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list for matlab type input\n",
    "Sample_List=[]\n",
    "\n",
    "for i in range(1, 100+1):\n",
    "    Sample_List.append(f'roi_100_{i}')\n",
    "    print(len(Sample_List))\n",
    "    pd.DataFrame(Sample_List).to_csv(f'./R2.2/ROIList_100_res2mm.txt', sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the cerebellum ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "# Load the gradient file\n",
    "gradient_file = nib.load('result_cerebellumonly_gradient1__17853_nifti.nii')\n",
    "gradient = gradient_file.get_data()\n",
    "affine = gradient_file.affine\n",
    "\n",
    "# Find non-zero values and their indices in the gradient\n",
    "grid_list = np.nonzero(gradient)\n",
    "grad_values = gradient[grid_list]\n",
    "\n",
    "# Split gradient values into 10 bins\n",
    "bins = np.linspace(np.min(grad_values), np.max(grad_values), 100+1)\n",
    "digitized = np.digitize(grad_values, bins)\n",
    "\n",
    "# Create 10 ROIs based on the bins\n",
    "roi = np.zeros_like(gradient)\n",
    "for i in range(1, 100+1, 10):\n",
    "    # Get indices for the current bin\n",
    "    bin_indices = grid_list[0][digitized == i], grid_list[1][digitized == i], grid_list[2][digitized == i]\n",
    "    roi[bin_indices] = i\n",
    "roi_nifti = nib.Nifti1Image(roi, affine)\n",
    "nib.save(roi_nifti, f'./R2.2/roi_100_10together.nii')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化到大脑的连接值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn import image, surface, plotting, datasets\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import datasets\n",
    "fsaverage = datasets.fetch_surf_fsaverage()\n",
    "\n",
    "mesh_L = \"/n02dat01/users/ypwang/Gradient/Tool/Atlas/BN_Atlas_freesurfer/BN_Atlas_freesurfer/fsaverage/fsaverage_LR32k/fsaverage.L.inflated.32k_fs_LR.surf.gii\"\n",
    "mesh_R = \"/n02dat01/users/ypwang/Gradient/Tool/Atlas/BN_Atlas_freesurfer/BN_Atlas_freesurfer/fsaverage/fsaverage_LR32k/fsaverage.R.inflated.32k_fs_LR.surf.gii\"\n",
    "\n",
    "fig, axes = plt.subplots(10, 4, figsize=(12, 40), subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "for i,index in enumerate([10,30,60,90]):\n",
    "    print(f'ROI{index}')\n",
    "    volume_img = nib.load(f\"./R2.2/lesion_network/ROI100/roi_100_{index}_t_fwe_0.001.nii.gz\")\n",
    "    \n",
    "    surface_L = surface.vol_to_surf(volume_img, fsaverage['pial_left'])\n",
    "    surface_L[surface_L==0]=np.nan\n",
    "    plotting.plot_surf_stat_map(fsaverage['pial_left'], surface_L, \n",
    "                                cmap='RdBu_r',\n",
    "                                vmax=20,\n",
    "                                colorbar=False,\n",
    "                                hemi='left',\n",
    "                                bg_map=fsaverage.sulc_left, \n",
    "                                axes=axes[i, 0])\n",
    "    plotting.plot_surf_stat_map(fsaverage['pial_left'], surface_L, \n",
    "                                cmap='RdBu_r',\n",
    "                                vmax=20,\n",
    "                                colorbar=False,\n",
    "                                hemi='left',\n",
    "                                bg_map=fsaverage.sulc_left, \n",
    "                                view='medial',\n",
    "                                axes=axes[i, 1])\n",
    "    \n",
    "    surface_R = surface.vol_to_surf(volume_img, fsaverage['pial_right'])\n",
    "    surface_R[surface_R==0]=np.nan\n",
    "    plotting.plot_surf_stat_map(fsaverage['pial_right'], surface_R, \n",
    "                                cmap='RdBu_r',\n",
    "                                vmax=20,\n",
    "                                colorbar=False,\n",
    "                                hemi='right',\n",
    "                                bg_map=fsaverage.sulc_right, \n",
    "                                axes=axes[i, 2])\n",
    "    plotting.plot_surf_stat_map(fsaverage['pial_right'], surface_R, \n",
    "                                cmap='RdBu_r',\n",
    "                                vmax=20,\n",
    "                                colorbar=False,\n",
    "                                hemi='right',\n",
    "                                bg_map=fsaverage.sulc_right, \n",
    "                                view='medial',\n",
    "                                axes=axes[i, 3])\n",
    "\n",
    "# Remove unused subplots\n",
    "for i in range(10, 100+1, 10):\n",
    "    axes[i//10-1, 1].axis('off')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./R2.2/Output/roi_100_FC2cort_fwe.png',dpi=500, bbox_inches='tight')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from scipy import stats\n",
    "from nilearn import datasets, image, input_data, plotting\n",
    "import scipy\n",
    "from scipy import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "git_dir = '/n02dat01/users/ypwang/Gradient/GeneticGradient/Scripts/'\n",
    "import sys\n",
    "sys.path.insert(0,git_dir)\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport FunctionLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGB_to_Hex(rgb):\n",
    "    RGB = rgb.split(',')\n",
    "    color = '#'\n",
    "    for i in RGB:\n",
    "        num = int(i)\n",
    "        color += str(hex(num))[-2:].replace('x', '0').upper()\n",
    "    return color\n",
    "yeo_dir = '/n02dat01/users/ypwang/Gradient/Tool/Atlas/Yeo2011_fcMRI_clustering/1000subjects_reference/'  \n",
    "\n",
    "# color_7 = pd.read_csv(\"/n01dat01/ypwang/AHBA/CerebellarGeneFCCorrelation/Reference_files/7NetworksColors.csv\")\n",
    "mat_struct = scipy.io.loadmat(f\"{yeo_dir}/7NetworksColors.mat\")\n",
    "color_7 = pd.DataFrame(mat_struct['colors']).drop(0)\n",
    "color_7.columns = ['R', 'G', 'B']\n",
    "index_7 = pd.read_csv(\"/n01dat01/ypwang/AHBA/CerebellarGeneFCCorrelation/Reference_files/7network_names.csv\")\n",
    "# index_7 = pd.read_csv(f\"{yeo_dir}/7NetworksOrderedNames.csv\", index_col=0)\n",
    "color_7_hex = [RGB_to_Hex(str(tuple(color_7.iloc[i,:])).replace(\"(\",\"\").replace(\")\",\"\")) for i in range(len(color_7))]\n",
    "color_7_hex = pd.DataFrame(color_7_hex)\n",
    "color_7.index = color_7_hex.index \n",
    "color_7 = pd.merge(color_7_hex,color_7,left_index=True,right_index=True)\n",
    "color_7 = pd.merge(index_7,color_7,left_index=True,right_index=True)\n",
    "color_7.columns = ['Name', 'Hex', 'R', 'G', 'B']\n",
    "\n",
    "order = ['SomMot', 'Vis', 'DorsAttn', 'VentAttn', 'Cont', 'Limbic', 'Default']\n",
    "    \n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "Net_7 = nib.load(\"/n02dat01/users/ypwang/Gradient/Tool/Atlas/Yeo2011_fcMRI_clustering/1000subjects_reference/Yeo_JNeurophysiol11_SplitLabels/MNI152/Yeo2011_7Networks_N1000.split_components.FSL_MNI152_2mm.nii.gz\")\n",
    "Net_7 = NiftiLabelsMasker(labels_img=Net_7, standardize=False, strategy=\"mean\")\n",
    "Net_7.fit()\n",
    "Net_7_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decile = 10\n",
    "N_name = pd.read_csv(f\"/n01dat01/ypwang/AHBA/CerebellarGeneFCCorrelation/Data/Yeo_JNeurophysiol11_SplitLabels/Yeo_JNeurophysiol11_SplitLabels/Yeo2011_7networks_N1000.split_components.glossary.csv\")\n",
    "    \n",
    "# plt.close()\n",
    "# fig,ax= plt.subplots(10,1, figsize=(10, 60)) #, gridspec_kw={'height_ratios': [1, 1.5, 1, 1.5, 1, 1.5, 1, 1.5]})\n",
    "# plt.tight_layout(pad=0, w_pad=0, h_pad=0.9)\n",
    "# fig.subplots_adjust(hspace=0.4)\n",
    "for id,index in enumerate(range(1, decile+1)):\n",
    "    FC_name =  nib.load(f\"./R2.2/lesion_network/roi_{decile}_{index}_t_fwe_0.001.nii.gz\")\n",
    "    FC_net7 = np.array(Net_7.transform(FC_name)).squeeze()\n",
    "\n",
    "    N_comparision = N_name.iloc[1:,:]\n",
    "    R = pd.DataFrame(FC_net7)\n",
    "    R.index = N_comparision.index\n",
    "    R.columns = ['R']\n",
    "    N_comparision = pd.concat([R, R, R, N_comparision], axis= 1)\n",
    "\n",
    "    # net 7\n",
    "    for i in range(len(N_comparision)):\n",
    "        N_comparision.iloc[i,1] = N_comparision.iloc[i,3].split('_')[-2]\n",
    "        if (N_comparision.iloc[i,1] in ['LH', 'RH']):\n",
    "            N_comparision.iloc[i,1] = N_comparision.iloc[i,3].split('_')[-1]\n",
    "        if ('SalVentAttn' in N_comparision.iloc[i,1]):\n",
    "            N_comparision.iloc[i,1] = N_comparision.iloc[i,1].replace('SalVentAttn', 'VentAttn')\n",
    "    N_comparision.columns = ['Net_index', 'Name', 'R', 'Label_Name', 'Network_Name','Full_component_name']\n",
    "\n",
    "    for i in np.unique(N_comparision.loc[:,'Name'].values):\n",
    "        for j in range(len(N_comparision)):\n",
    "            if N_comparision.iloc[j,1]== i:\n",
    "                N_comparision.iloc[j,0]= int(index_7[index_7.loc[:,'None']==i].index.values)\n",
    "    N_comparision = N_comparision.sort_values(by='Net_index')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decile = 100\n",
    "num_iterations = decile\n",
    "matrix_7 = np.zeros((7, num_iterations))\n",
    "for id,index in enumerate(range(1, decile+1)):\n",
    "    FC_name =  nib.load(f\"./R2.2/lesion_network/ROI100/roi_{decile}_{index}_t_fwe_0.001.nii.gz\")\n",
    "    FC_net7 = np.array(Net_7.transform(FC_name)).squeeze()\n",
    "\n",
    "    N_comparision = N_name.iloc[1:,:]\n",
    "    R = pd.DataFrame(FC_net7)\n",
    "    R.index = N_comparision.index\n",
    "    R.columns = ['R']\n",
    "    N_comparision = pd.concat([R, R, R, N_comparision], axis= 1)\n",
    "\n",
    "    # net 7\n",
    "    for i in range(len(N_comparision)):\n",
    "        N_comparision.iloc[i,1] = N_comparision.iloc[i,3].split('_')[-2]\n",
    "        if (N_comparision.iloc[i,1] in ['LH', 'RH']):\n",
    "            N_comparision.iloc[i,1] = N_comparision.iloc[i,3].split('_')[-1]\n",
    "        if ('SalVentAttn' in N_comparision.iloc[i,1]):\n",
    "            N_comparision.iloc[i,1] = N_comparision.iloc[i,1].replace('SalVentAttn', 'VentAttn')\n",
    "    N_comparision.columns = ['Net_index', 'Name', 'R', 'Label_Name', 'Network_Name','Full_component_name']\n",
    "\n",
    "    for i in np.unique(N_comparision.loc[:,'Name'].values):\n",
    "        for j in range(len(N_comparision)):\n",
    "            if N_comparision.iloc[j,1]== i:\n",
    "                N_comparision.iloc[j,0]= int(index_7[index_7.loc[:,'None']==i].index.values)\n",
    "    N_comparision = N_comparision.sort_values(by='Net_index')\n",
    "\n",
    "    \n",
    "    N_cur = N_comparision.groupby('Name').mean()\n",
    "    matrix_7[:, index - 1] = N_cur['R'].values\n",
    "matrix_7 = pd.DataFrame(matrix_7)\n",
    "matrix_7.index = N_cur.index\n",
    "matrix_7.columns = range(1, decile+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as mp, seaborn\n",
    "\n",
    "# Define the desired row order\n",
    "row_order = ['SomMot', 'Vis', 'DorsAttn', 'VentAttn', 'Cont', 'Limbic', 'Default']\n",
    "\n",
    "# Create a DataFrame with the matrix_data and row_order\n",
    "heatmap_df = pd.DataFrame(matrix_7, index=row_order)\n",
    "\n",
    "# Plot the heatmap\n",
    "seaborn.heatmap(heatmap_df.T, center=0, cmap=\"RdBu_r\")\n",
    "plt.yticks(range(10, 101, 10), labels=range(10, 101, 10), rotation=0)\n",
    "plt.title('Seneorimotor to Association')\n",
    "plt.savefig(f'./R2.2/Output/roi_{decile}_FC2cort_heatmap_net7.png',dpi=500, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradient",
   "language": "python",
   "name": "gradient"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
